{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f5eb16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:36:32.539068Z",
     "iopub.status.busy": "2022-05-28T07:36:32.538440Z",
     "iopub.status.idle": "2022-05-28T07:36:33.688331Z",
     "shell.execute_reply": "2022-05-28T07:36:33.687029Z"
    },
    "papermill": {
     "duration": 1.163784,
     "end_time": "2022-05-28T07:36:33.691100",
     "exception": false,
     "start_time": "2022-05-28T07:36:32.527316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3171241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:36:33.705613Z",
     "iopub.status.busy": "2022-05-28T07:36:33.704865Z",
     "iopub.status.idle": "2022-05-28T07:36:48.848911Z",
     "shell.execute_reply": "2022-05-28T07:36:48.847907Z"
    },
    "papermill": {
     "duration": 15.153811,
     "end_time": "2022-05-28T07:36:48.851331",
     "exception": false,
     "start_time": "2022-05-28T07:36:33.697520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/tabular-playground-series-may-2022/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/tabular-playground-series-may-2022/test.csv\")\n",
    "id1 = df_train[\"id\"]\n",
    "id2 = df_test[\"id\"]\n",
    "df_train.drop(columns=[\"id\"], inplace=True)\n",
    "df_test.drop(columns=[\"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c58e969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:36:48.865750Z",
     "iopub.status.busy": "2022-05-28T07:36:48.865382Z",
     "iopub.status.idle": "2022-05-28T07:36:49.077239Z",
     "shell.execute_reply": "2022-05-28T07:36:49.075705Z"
    },
    "papermill": {
     "duration": 0.222204,
     "end_time": "2022-05-28T07:36:49.079921",
     "exception": false,
     "start_time": "2022-05-28T07:36:48.857717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "sfk = StratifiedKFold()\n",
    "def folds(df, set_):\n",
    "    df[\"kfold\"] = -1\n",
    "    if set_ == \"train\":\n",
    "        df[\"id\"] = id1\n",
    "        for f , (tid, vid) in enumerate(sfk.split(df, df[\"target\"])):\n",
    "            df.loc[vid, \"kfold\"] = f\n",
    "    if set_ == \"test\":\n",
    "        df[\"id\"] = id2\n",
    "        df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8212e577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:36:49.095995Z",
     "iopub.status.busy": "2022-05-28T07:36:49.095183Z",
     "iopub.status.idle": "2022-05-28T07:36:49.316761Z",
     "shell.execute_reply": "2022-05-28T07:36:49.315976Z"
    },
    "papermill": {
     "duration": 0.232811,
     "end_time": "2022-05-28T07:36:49.319108",
     "exception": false,
     "start_time": "2022-05-28T07:36:49.086297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_folds = folds(df_train, \"train\")\n",
    "test = folds(df_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f397ce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:36:49.333263Z",
     "iopub.status.busy": "2022-05-28T07:36:49.332560Z",
     "iopub.status.idle": "2022-05-28T07:36:49.614033Z",
     "shell.execute_reply": "2022-05-28T07:36:49.613061Z"
    },
    "papermill": {
     "duration": 0.290971,
     "end_time": "2022-05-28T07:36:49.616298",
     "exception": false,
     "start_time": "2022-05-28T07:36:49.325327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>...</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>kfold</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.442517</td>\n",
       "      <td>0.174380</td>\n",
       "      <td>-0.999816</td>\n",
       "      <td>0.762741</td>\n",
       "      <td>0.186778</td>\n",
       "      <td>-1.074775</td>\n",
       "      <td>0.501888</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.435736</td>\n",
       "      <td>-2.427430</td>\n",
       "      <td>-1.966887</td>\n",
       "      <td>5.734205</td>\n",
       "      <td>BAAABADLAC</td>\n",
       "      <td>99.478419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.605598</td>\n",
       "      <td>-0.305715</td>\n",
       "      <td>0.627667</td>\n",
       "      <td>-0.578898</td>\n",
       "      <td>-1.750931</td>\n",
       "      <td>1.355550</td>\n",
       "      <td>-0.190911</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.883322</td>\n",
       "      <td>-2.848714</td>\n",
       "      <td>-0.725155</td>\n",
       "      <td>3.194219</td>\n",
       "      <td>AFABBAEGCB</td>\n",
       "      <td>-65.993825</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>900001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.303990</td>\n",
       "      <td>2.445110</td>\n",
       "      <td>0.246515</td>\n",
       "      <td>0.818248</td>\n",
       "      <td>0.359731</td>\n",
       "      <td>-1.331845</td>\n",
       "      <td>1.358622</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.157192</td>\n",
       "      <td>1.714005</td>\n",
       "      <td>0.585032</td>\n",
       "      <td>0.066898</td>\n",
       "      <td>BBACABBKEE</td>\n",
       "      <td>-87.405622</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>900002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.154053</td>\n",
       "      <td>0.260126</td>\n",
       "      <td>-1.367092</td>\n",
       "      <td>-0.093175</td>\n",
       "      <td>-1.111034</td>\n",
       "      <td>-0.948481</td>\n",
       "      <td>1.119220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.754570</td>\n",
       "      <td>-2.364007</td>\n",
       "      <td>-1.003320</td>\n",
       "      <td>3.893099</td>\n",
       "      <td>AEBEAACQCC</td>\n",
       "      <td>-281.293460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>900003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.651904</td>\n",
       "      <td>-0.424266</td>\n",
       "      <td>-0.667356</td>\n",
       "      <td>-0.322124</td>\n",
       "      <td>-0.089462</td>\n",
       "      <td>0.181705</td>\n",
       "      <td>1.784983</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130467</td>\n",
       "      <td>-3.557893</td>\n",
       "      <td>1.210687</td>\n",
       "      <td>1.861884</td>\n",
       "      <td>AEBBBBDABF</td>\n",
       "      <td>25.629415</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>900004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699995</th>\n",
       "      <td>0.640110</td>\n",
       "      <td>0.897808</td>\n",
       "      <td>-0.523956</td>\n",
       "      <td>1.563760</td>\n",
       "      <td>-0.092281</td>\n",
       "      <td>-0.610867</td>\n",
       "      <td>0.535426</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518110</td>\n",
       "      <td>1.243837</td>\n",
       "      <td>0.575111</td>\n",
       "      <td>0.076372</td>\n",
       "      <td>BCBCEBHMCD</td>\n",
       "      <td>204.186539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1599995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699996</th>\n",
       "      <td>-0.191771</td>\n",
       "      <td>-0.035246</td>\n",
       "      <td>-0.118533</td>\n",
       "      <td>0.584750</td>\n",
       "      <td>2.126977</td>\n",
       "      <td>0.568659</td>\n",
       "      <td>-0.052663</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.135740</td>\n",
       "      <td>2.982713</td>\n",
       "      <td>-1.511760</td>\n",
       "      <td>2.225218</td>\n",
       "      <td>BAABCADQFC</td>\n",
       "      <td>-97.694591</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1599996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699997</th>\n",
       "      <td>-0.331704</td>\n",
       "      <td>-0.328845</td>\n",
       "      <td>-1.185503</td>\n",
       "      <td>1.022128</td>\n",
       "      <td>-0.483099</td>\n",
       "      <td>-0.107146</td>\n",
       "      <td>-0.968281</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.096011</td>\n",
       "      <td>-2.734508</td>\n",
       "      <td>-4.885955</td>\n",
       "      <td>-2.248739</td>\n",
       "      <td>AAAJCBGQBA</td>\n",
       "      <td>130.622745</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1599997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699998</th>\n",
       "      <td>-2.031073</td>\n",
       "      <td>-1.238398</td>\n",
       "      <td>0.964699</td>\n",
       "      <td>-1.045950</td>\n",
       "      <td>0.906064</td>\n",
       "      <td>0.634301</td>\n",
       "      <td>-0.707474</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018995</td>\n",
       "      <td>1.973697</td>\n",
       "      <td>-0.353068</td>\n",
       "      <td>-3.333449</td>\n",
       "      <td>BCBBCABNDE</td>\n",
       "      <td>-364.625148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699999</th>\n",
       "      <td>-0.085906</td>\n",
       "      <td>-0.002124</td>\n",
       "      <td>2.227375</td>\n",
       "      <td>0.217145</td>\n",
       "      <td>3.179153</td>\n",
       "      <td>-1.660188</td>\n",
       "      <td>0.891989</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.017221</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>-3.236026</td>\n",
       "      <td>-0.362070</td>\n",
       "      <td>AFBEBACHFF</td>\n",
       "      <td>-155.417342</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1599999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700000 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
       "0       0.442517  0.174380 -0.999816  0.762741  0.186778 -1.074775  0.501888   \n",
       "1      -0.605598 -0.305715  0.627667 -0.578898 -1.750931  1.355550 -0.190911   \n",
       "2       0.303990  2.445110  0.246515  0.818248  0.359731 -1.331845  1.358622   \n",
       "3       0.154053  0.260126 -1.367092 -0.093175 -1.111034 -0.948481  1.119220   \n",
       "4      -1.651904 -0.424266 -0.667356 -0.322124 -0.089462  0.181705  1.784983   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "699995  0.640110  0.897808 -0.523956  1.563760 -0.092281 -0.610867  0.535426   \n",
       "699996 -0.191771 -0.035246 -0.118533  0.584750  2.126977  0.568659 -0.052663   \n",
       "699997 -0.331704 -0.328845 -1.185503  1.022128 -0.483099 -0.107146 -0.968281   \n",
       "699998 -2.031073 -1.238398  0.964699 -1.045950  0.906064  0.634301 -0.707474   \n",
       "699999 -0.085906 -0.002124  2.227375  0.217145  3.179153 -1.660188  0.891989   \n",
       "\n",
       "        f_07  f_08  f_09  ...      f_23      f_24      f_25      f_26  \\\n",
       "0          6     6     0  ... -2.435736 -2.427430 -1.966887  5.734205   \n",
       "1          1     3     4  ...  1.883322 -2.848714 -0.725155  3.194219   \n",
       "2          3     3     4  ... -5.157192  1.714005  0.585032  0.066898   \n",
       "3          0     0     4  ...  1.754570 -2.364007 -1.003320  3.893099   \n",
       "4          2     2     2  ... -0.130467 -3.557893  1.210687  1.861884   \n",
       "...      ...   ...   ...  ...       ...       ...       ...       ...   \n",
       "699995     0     1     6  ...  0.518110  1.243837  0.575111  0.076372   \n",
       "699996     4     3     4  ... -1.135740  2.982713 -1.511760  2.225218   \n",
       "699997     1     1     2  ...  1.096011 -2.734508 -4.885955 -2.248739   \n",
       "699998     5     1     1  ...  1.018995  1.973697 -0.353068 -3.333449   \n",
       "699999     0     3     4  ... -5.017221  0.251268 -3.236026 -0.362070   \n",
       "\n",
       "              f_27        f_28  f_29  f_30  kfold       id  \n",
       "0       BAAABADLAC   99.478419     0     0     -1   900000  \n",
       "1       AFABBAEGCB  -65.993825     1     0     -1   900001  \n",
       "2       BBACABBKEE  -87.405622     0     1     -1   900002  \n",
       "3       AEBEAACQCC -281.293460     0     0     -1   900003  \n",
       "4       AEBBBBDABF   25.629415     0     2     -1   900004  \n",
       "...            ...         ...   ...   ...    ...      ...  \n",
       "699995  BCBCEBHMCD  204.186539     0     0     -1  1599995  \n",
       "699996  BAABCADQFC  -97.694591     0     2     -1  1599996  \n",
       "699997  AAAJCBGQBA  130.622745     1     0     -1  1599997  \n",
       "699998  BCBBCABNDE -364.625148     0     0     -1  1599998  \n",
       "699999  AFBEBACHFF -155.417342     0     1     -1  1599999  \n",
       "\n",
       "[700000 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a7271c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:36:49.633255Z",
     "iopub.status.busy": "2022-05-28T07:36:49.631473Z",
     "iopub.status.idle": "2022-05-28T07:36:49.639205Z",
     "shell.execute_reply": "2022-05-28T07:36:49.638169Z"
    },
    "papermill": {
     "duration": 0.017962,
     "end_time": "2022-05-28T07:36:49.641356",
     "exception": false,
     "start_time": "2022-05-28T07:36:49.623394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reindex_df(df, excluding_columns):\n",
    "    id_ = df.iloc[:, -excluding_columns:]\n",
    "    cat = df.loc[:, df.dtypes==\"int64\"].iloc[:, :-excluding_columns]\n",
    "    cont = df.loc[:, df.dtypes==\"float64\"]\n",
    "    f27 = df.loc[:, df.dtypes==\"object\"]\n",
    "    return pd.concat([id_,cat, cont, f27], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b215a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:36:49.657136Z",
     "iopub.status.busy": "2022-05-28T07:36:49.656443Z",
     "iopub.status.idle": "2022-05-28T07:36:50.224485Z",
     "shell.execute_reply": "2022-05-28T07:36:50.223546Z"
    },
    "papermill": {
     "duration": 0.578481,
     "end_time": "2022-05-28T07:36:50.226905",
     "exception": false,
     "start_time": "2022-05-28T07:36:49.648424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_folds = reindex_df(train_folds, 3)\n",
    "test = reindex_df(test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9df400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:36:50.241984Z",
     "iopub.status.busy": "2022-05-28T07:36:50.241615Z",
     "iopub.status.idle": "2022-05-28T07:36:50.251000Z",
     "shell.execute_reply": "2022-05-28T07:36:50.250239Z"
    },
    "papermill": {
     "duration": 0.019271,
     "end_time": "2022-05-28T07:36:50.253020",
     "exception": false,
     "start_time": "2022-05-28T07:36:50.233749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alphabet dictionary\n",
    "import string\n",
    "i = 0\n",
    "range_ = list(range(1,27))\n",
    "alphabets = dict.fromkeys(string.ascii_uppercase,)\n",
    "for name, key in alphabets.items():\n",
    "    alphabets[name] = range_[i]\n",
    "    i = i + 1\n",
    "\n",
    "def f27_features(f27):\n",
    "    features_split = []\n",
    "    for id_ in f27.index:\n",
    "        string = list(f27.loc[id_,\"f_27\"])\n",
    "        features_split.append(string)\n",
    "\n",
    "    encoded_features = []\n",
    "    for feature in features_split:\n",
    "        encoded = []\n",
    "        for char in feature:\n",
    "            encoded.append(alphabets[char])\n",
    "        encoded_features.append(encoded)\n",
    "\n",
    "    col_names = []\n",
    "    for i in range(10):\n",
    "        name = \"f27_\" + str(i)\n",
    "        col_names.append(name)\n",
    "    new_features = pd.DataFrame(encoded_features, columns=col_names)\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb2a235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:36:50.268418Z",
     "iopub.status.busy": "2022-05-28T07:36:50.268008Z",
     "iopub.status.idle": "2022-05-28T07:37:09.829326Z",
     "shell.execute_reply": "2022-05-28T07:37:09.828240Z"
    },
    "papermill": {
     "duration": 19.571852,
     "end_time": "2022-05-28T07:37:09.831727",
     "exception": false,
     "start_time": "2022-05-28T07:36:50.259875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_features = f27_features(train_folds)\n",
    "train_folds = pd.concat([train_folds, new_features], axis=1)\n",
    "train_folds.drop(columns=[\"f_27\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6fe6513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:37:09.847013Z",
     "iopub.status.busy": "2022-05-28T07:37:09.846237Z",
     "iopub.status.idle": "2022-05-28T07:37:10.886858Z",
     "shell.execute_reply": "2022-05-28T07:37:10.885878Z"
    },
    "papermill": {
     "duration": 1.050734,
     "end_time": "2022-05-28T07:37:10.889200",
     "exception": false,
     "start_time": "2022-05-28T07:37:09.838466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightgbm\n",
    "from sklearn.metrics import accuracy_score\n",
    "def objective(trial, df):\n",
    "    params = {\n",
    "    'objective':'binary',\n",
    "    'metric' : 'binary_error',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': trial.suggest_int(\"num_leaves\",2 , 256),\n",
    "    'feature_fraction': trial.suggest_float(\"feature_fraction\", 0.1, 1.0),\n",
    "    'bagging_fraction': trial.suggest_float(\"bagging_fraction\", 0.1, 1.0),\n",
    "    'learning_rate': trial.suggest_float(\"learning_rate\", 0.01,1) }\n",
    "    accuracy = []\n",
    "    for fold in range(5):\n",
    "        train = df[df[\"kfold\"] != fold]\n",
    "        valid = df[df[\"kfold\"] == fold]\n",
    "        train_lgbm = lightgbm.Dataset(train.iloc[:,3:].to_numpy(), train[\"target\"].to_numpy())\n",
    "        model_lgbm = lightgbm.train(params, train_lgbm, num_boost_round=200)\n",
    "        pred_prob = model_lgbm.predict(valid.iloc[:,3:].to_numpy())\n",
    "        y_pred = []\n",
    "        for pred in pred_prob:\n",
    "            if pred >= 0.5:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        ypred = np.array(y_pred)\n",
    "        fold_accuracy = accuracy_score(valid[\"target\"].to_numpy(), ypred)\n",
    "        accuracy.append(fold_accuracy)\n",
    "        return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91185526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:37:10.904501Z",
     "iopub.status.busy": "2022-05-28T07:37:10.904118Z",
     "iopub.status.idle": "2022-05-28T07:37:10.908730Z",
     "shell.execute_reply": "2022-05-28T07:37:10.907871Z"
    },
    "papermill": {
     "duration": 0.014583,
     "end_time": "2022-05-28T07:37:10.910686",
     "exception": false,
     "start_time": "2022-05-28T07:37:10.896103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "optimisation_func = partial(objective, df=train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06561824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T07:37:10.926420Z",
     "iopub.status.busy": "2022-05-28T07:37:10.925572Z",
     "iopub.status.idle": "2022-05-28T08:14:47.349682Z",
     "shell.execute_reply": "2022-05-28T08:14:47.348788Z"
    },
    "papermill": {
     "duration": 2256.434505,
     "end_time": "2022-05-28T08:14:47.352243",
     "exception": false,
     "start_time": "2022-05-28T07:37:10.917738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:37:11,645]\u001b[0m A new study created in memory with name: no-name-759252e7-500b-42af-82f7-d9d220e048f3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:37:28,189]\u001b[0m Trial 0 finished with value: 0.8897722222222222 and parameters: {'num_leaves': 177, 'feature_fraction': 0.31999749666241584, 'bagging_fraction': 0.6199122588388134, 'learning_rate': 0.1680817809171493}. Best is trial 0 with value: 0.8897722222222222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:37:39,046]\u001b[0m Trial 1 finished with value: 0.8460944444444445 and parameters: {'num_leaves': 4, 'feature_fraction': 0.5820646456046967, 'bagging_fraction': 0.6564109377014177, 'learning_rate': 0.8705374895181737}. Best is trial 0 with value: 0.8897722222222222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:37:59,804]\u001b[0m Trial 2 finished with value: 0.8990166666666667 and parameters: {'num_leaves': 213, 'feature_fraction': 0.8878905558093969, 'bagging_fraction': 0.13126560869934312, 'learning_rate': 0.8043383538166323}. Best is trial 2 with value: 0.8990166666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:38:20,535]\u001b[0m Trial 3 finished with value: 0.9128277777777778 and parameters: {'num_leaves': 172, 'feature_fraction': 0.7612109159392215, 'bagging_fraction': 0.42399687780926887, 'learning_rate': 0.3163099573716253}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:38:41,357]\u001b[0m Trial 4 finished with value: 0.9046555555555555 and parameters: {'num_leaves': 221, 'feature_fraction': 0.520033914085385, 'bagging_fraction': 0.18206142018446375, 'learning_rate': 0.17680025416805778}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:38:53,294]\u001b[0m Trial 5 finished with value: 0.855 and parameters: {'num_leaves': 167, 'feature_fraction': 0.1605586166469079, 'bagging_fraction': 0.2143123961978835, 'learning_rate': 0.6211419348150526}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:39:15,442]\u001b[0m Trial 6 finished with value: 0.9111555555555556 and parameters: {'num_leaves': 249, 'feature_fraction': 0.8025726380767412, 'bagging_fraction': 0.6794386396266743, 'learning_rate': 0.49793671496807845}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:39:28,564]\u001b[0m Trial 7 finished with value: 0.8912888888888889 and parameters: {'num_leaves': 20, 'feature_fraction': 0.8347253856110889, 'bagging_fraction': 0.32347049537671674, 'learning_rate': 0.4382533916020496}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:39:41,817]\u001b[0m Trial 8 finished with value: 0.8853722222222222 and parameters: {'num_leaves': 62, 'feature_fraction': 0.3431717893076453, 'bagging_fraction': 0.9960122810885524, 'learning_rate': 0.18870743206584206}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:40:02,563]\u001b[0m Trial 9 finished with value: 0.8871666666666667 and parameters: {'num_leaves': 196, 'feature_fraction': 0.7922690896828668, 'bagging_fraction': 0.12540954601155507, 'learning_rate': 0.8897617975065555}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:40:29,253]\u001b[0m Trial 10 finished with value: 0.8600777777777778 and parameters: {'num_leaves': 102, 'feature_fraction': 0.9898886514343217, 'bagging_fraction': 0.40710285278156855, 'learning_rate': 0.021692339488278478}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:40:51,308]\u001b[0m Trial 11 finished with value: 0.9072944444444444 and parameters: {'num_leaves': 252, 'feature_fraction': 0.6673782432135126, 'bagging_fraction': 0.7985036633971649, 'learning_rate': 0.4660945515359505}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:41:10,138]\u001b[0m Trial 12 finished with value: 0.9095 and parameters: {'num_leaves': 138, 'feature_fraction': 0.6993659199046052, 'bagging_fraction': 0.5131333946447494, 'learning_rate': 0.35439111201073437}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:41:31,652]\u001b[0m Trial 13 finished with value: 0.9086166666666666 and parameters: {'num_leaves': 249, 'feature_fraction': 0.9871019911700807, 'bagging_fraction': 0.8109635633869265, 'learning_rate': 0.6170301929383231}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:41:50,452]\u001b[0m Trial 14 finished with value: 0.9108888888888889 and parameters: {'num_leaves': 145, 'feature_fraction': 0.7419661470372949, 'bagging_fraction': 0.4853890444923816, 'learning_rate': 0.3349000875410857}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:42:05,497]\u001b[0m Trial 15 finished with value: 0.8975555555555556 and parameters: {'num_leaves': 101, 'feature_fraction': 0.5552402625245597, 'bagging_fraction': 0.7318392623387149, 'learning_rate': 0.5967350087960124}. Best is trial 3 with value: 0.9128277777777778.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:42:26,465]\u001b[0m Trial 16 finished with value: 0.9173 and parameters: {'num_leaves': 229, 'feature_fraction': 0.8618438231022202, 'bagging_fraction': 0.3270261377655123, 'learning_rate': 0.31855089489177546}. Best is trial 16 with value: 0.9173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:42:51,400]\u001b[0m Trial 17 finished with value: 0.9160055555555555 and parameters: {'num_leaves': 178, 'feature_fraction': 0.8893147859659272, 'bagging_fraction': 0.3150745378747355, 'learning_rate': 0.28139580971613387}. Best is trial 16 with value: 0.9173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:43:24,565]\u001b[0m Trial 18 finished with value: 0.8639666666666667 and parameters: {'num_leaves': 217, 'feature_fraction': 0.8818944644066316, 'bagging_fraction': 0.2817015137138739, 'learning_rate': 0.016482010818262227}. Best is trial 16 with value: 0.9173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:43:47,101]\u001b[0m Trial 19 finished with value: 0.9176611111111111 and parameters: {'num_leaves': 193, 'feature_fraction': 0.9188570206846403, 'bagging_fraction': 0.3334046476329954, 'learning_rate': 0.2491594237079432}. Best is trial 19 with value: 0.9176611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:44:01,977]\u001b[0m Trial 20 finished with value: 0.8940944444444444 and parameters: {'num_leaves': 110, 'feature_fraction': 0.4244572152765182, 'bagging_fraction': 0.3759279546837521, 'learning_rate': 0.23057413552408598}. Best is trial 19 with value: 0.9176611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:44:24,026]\u001b[0m Trial 21 finished with value: 0.9180611111111111 and parameters: {'num_leaves': 192, 'feature_fraction': 0.9113753579559056, 'bagging_fraction': 0.2695202500483962, 'learning_rate': 0.2716053975931823}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:44:51,153]\u001b[0m Trial 22 finished with value: 0.9160333333333334 and parameters: {'num_leaves': 204, 'feature_fraction': 0.9501395249486101, 'bagging_fraction': 0.21592586216183768, 'learning_rate': 0.11793229302738678}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:45:12,254]\u001b[0m Trial 23 finished with value: 0.9073888888888889 and parameters: {'num_leaves': 235, 'feature_fraction': 0.6439009063321073, 'bagging_fraction': 0.2738656076281365, 'learning_rate': 0.4138395770400174}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:45:40,215]\u001b[0m Trial 24 finished with value: 0.9121833333333333 and parameters: {'num_leaves': 192, 'feature_fraction': 0.8986711418091421, 'bagging_fraction': 0.45175278601970215, 'learning_rate': 0.10566165096289407}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:46:03,871]\u001b[0m Trial 25 finished with value: 0.9179111111111111 and parameters: {'num_leaves': 233, 'feature_fraction': 0.8378357905085421, 'bagging_fraction': 0.5792496389248951, 'learning_rate': 0.27177090362956596}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:46:23,386]\u001b[0m Trial 26 finished with value: 0.9156111111111112 and parameters: {'num_leaves': 154, 'feature_fraction': 0.9534945995690141, 'bagging_fraction': 0.6063779442782341, 'learning_rate': 0.24961838663303218}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:46:42,044]\u001b[0m Trial 27 finished with value: 0.9080333333333334 and parameters: {'num_leaves': 125, 'feature_fraction': 0.7377562261817774, 'bagging_fraction': 0.5561618656370041, 'learning_rate': 0.5576776322671602}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:46:52,231]\u001b[0m Trial 28 finished with value: 0.8156055555555556 and parameters: {'num_leaves': 196, 'feature_fraction': 0.10005823159749172, 'bagging_fraction': 0.5401839733759441, 'learning_rate': 0.7614211938830551}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:47:12,751]\u001b[0m Trial 29 finished with value: 0.9090111111111111 and parameters: {'num_leaves': 184, 'feature_fraction': 0.6218913112290665, 'bagging_fraction': 0.3824283935548887, 'learning_rate': 0.37451037743566573}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:47:29,880]\u001b[0m Trial 30 finished with value: 0.8996833333333333 and parameters: {'num_leaves': 156, 'feature_fraction': 0.4981658701830122, 'bagging_fraction': 0.6002323582807222, 'learning_rate': 0.21879213971280329}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:47:58,022]\u001b[0m Trial 31 finished with value: 0.9143666666666667 and parameters: {'num_leaves': 224, 'feature_fraction': 0.8329611716397759, 'bagging_fraction': 0.333432823527132, 'learning_rate': 0.11639794873169687}. Best is trial 21 with value: 0.9180611111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:48:21,504]\u001b[0m Trial 32 finished with value: 0.9189111111111111 and parameters: {'num_leaves': 234, 'feature_fraction': 0.9244249978272945, 'bagging_fraction': 0.24145638243610884, 'learning_rate': 0.29125800225505166}. Best is trial 32 with value: 0.9189111111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:48:43,812]\u001b[0m Trial 33 finished with value: 0.9185777777777778 and parameters: {'num_leaves': 237, 'feature_fraction': 0.9281420256342917, 'bagging_fraction': 0.18504174281905017, 'learning_rate': 0.2749240168390596}. Best is trial 32 with value: 0.9189111111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:49:05,963]\u001b[0m Trial 34 finished with value: 0.9152222222222223 and parameters: {'num_leaves': 238, 'feature_fraction': 0.9352874337873889, 'bagging_fraction': 0.1578376899369081, 'learning_rate': 0.41666636106329796}. Best is trial 32 with value: 0.9189111111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:49:31,149]\u001b[0m Trial 35 finished with value: 0.9179555555555555 and parameters: {'num_leaves': 210, 'feature_fraction': 0.9952038532328227, 'bagging_fraction': 0.24275297366787213, 'learning_rate': 0.14592029358398964}. Best is trial 32 with value: 0.9189111111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:49:51,399]\u001b[0m Trial 36 finished with value: 0.8912666666666667 and parameters: {'num_leaves': 212, 'feature_fraction': 0.9916409345806959, 'bagging_fraction': 0.23477764242155835, 'learning_rate': 0.9663942778826365}. Best is trial 32 with value: 0.9189111111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:50:20,572]\u001b[0m Trial 37 finished with value: 0.9045222222222222 and parameters: {'num_leaves': 210, 'feature_fraction': 0.8006868874706184, 'bagging_fraction': 0.1144613679829652, 'learning_rate': 0.07647673017866079}. Best is trial 32 with value: 0.9189111111111111.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:50:47,246]\u001b[0m Trial 38 finished with value: 0.9217666666666666 and parameters: {'num_leaves': 255, 'feature_fraction': 0.9646367375935679, 'bagging_fraction': 0.19053681610791118, 'learning_rate': 0.16513221476967985}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:51:04,672]\u001b[0m Trial 39 finished with value: 0.8732166666666666 and parameters: {'num_leaves': 253, 'feature_fraction': 0.23244193022854964, 'bagging_fraction': 0.20232419665230647, 'learning_rate': 0.1771841201986028}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:51:25,840]\u001b[0m Trial 40 finished with value: 0.9092777777777777 and parameters: {'num_leaves': 241, 'feature_fraction': 0.7618256804422467, 'bagging_fraction': 0.14937233593191868, 'learning_rate': 0.5251789661274577}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:51:51,999]\u001b[0m Trial 41 finished with value: 0.9172555555555556 and parameters: {'num_leaves': 223, 'feature_fraction': 0.9479104259610902, 'bagging_fraction': 0.22955602507210546, 'learning_rate': 0.14223014219361355}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:52:17,878]\u001b[0m Trial 42 finished with value: 0.9186277777777778 and parameters: {'num_leaves': 242, 'feature_fraction': 0.9189689379090389, 'bagging_fraction': 0.2764755000209914, 'learning_rate': 0.18607443780368466}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:52:49,643]\u001b[0m Trial 43 finished with value: 0.9062722222222223 and parameters: {'num_leaves': 242, 'feature_fraction': 0.8561695148664127, 'bagging_fraction': 0.17271452460668832, 'learning_rate': 0.0685442206708747}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:53:14,448]\u001b[0m Trial 44 finished with value: 0.9200666666666667 and parameters: {'num_leaves': 251, 'feature_fraction': 0.9178597830837907, 'bagging_fraction': 0.1008597606633252, 'learning_rate': 0.2055663208798501}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:53:39,380]\u001b[0m Trial 45 finished with value: 0.9168333333333333 and parameters: {'num_leaves': 256, 'feature_fraction': 0.7968931891415146, 'bagging_fraction': 0.12324350590127782, 'learning_rate': 0.19575914298771727}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054654 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:53:57,224]\u001b[0m Trial 46 finished with value: 0.9020722222222223 and parameters: {'num_leaves': 56, 'feature_fraction': 0.9570997141576603, 'bagging_fraction': 0.10167220828425247, 'learning_rate': 0.18060976117688043}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:54:19,698]\u001b[0m Trial 47 finished with value: 0.9126277777777778 and parameters: {'num_leaves': 243, 'feature_fraction': 0.6985177446371278, 'bagging_fraction': 0.20034732010384934, 'learning_rate': 0.38331153368567183}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:54:40,465]\u001b[0m Trial 48 finished with value: 0.9035 and parameters: {'num_leaves': 230, 'feature_fraction': 0.8674981415024337, 'bagging_fraction': 0.15315829847697177, 'learning_rate': 0.7051630836708151}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:55:12,190]\u001b[0m Trial 49 finished with value: 0.9048666666666667 and parameters: {'num_leaves': 225, 'feature_fraction': 0.9248636483782443, 'bagging_fraction': 0.28231067942416377, 'learning_rate': 0.06328461748115369}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:55:36,038]\u001b[0m Trial 50 finished with value: 0.9169833333333334 and parameters: {'num_leaves': 254, 'feature_fraction': 0.8261998738142582, 'bagging_fraction': 0.18734915131665966, 'learning_rate': 0.30654994626779447}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:55:58,451]\u001b[0m Trial 51 finished with value: 0.9186055555555556 and parameters: {'num_leaves': 244, 'feature_fraction': 0.9084003618312504, 'bagging_fraction': 0.2597063640263643, 'learning_rate': 0.3149319645355429}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:56:19,446]\u001b[0m Trial 52 finished with value: 0.9174777777777777 and parameters: {'num_leaves': 241, 'feature_fraction': 0.8838834133722486, 'bagging_fraction': 0.9256563599520209, 'learning_rate': 0.31859072959987483}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:56:44,842]\u001b[0m Trial 53 finished with value: 0.9208611111111111 and parameters: {'num_leaves': 256, 'feature_fraction': 0.9756995575822681, 'bagging_fraction': 0.2510236327956546, 'learning_rate': 0.2043480413536285}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:57:09,418]\u001b[0m Trial 54 finished with value: 0.9209 and parameters: {'num_leaves': 256, 'feature_fraction': 0.9720630754828006, 'bagging_fraction': 0.369332377091787, 'learning_rate': 0.21431734154018273}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:57:33,975]\u001b[0m Trial 55 finished with value: 0.9212111111111111 and parameters: {'num_leaves': 256, 'feature_fraction': 0.9674685886935271, 'bagging_fraction': 0.38048475741214854, 'learning_rate': 0.21168545766358482}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:57:56,965]\u001b[0m Trial 56 finished with value: 0.9201333333333334 and parameters: {'num_leaves': 256, 'feature_fraction': 0.9717719169954964, 'bagging_fraction': 0.45634250772597434, 'learning_rate': 0.21839249755079332}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:58:21,031]\u001b[0m Trial 57 finished with value: 0.9212666666666667 and parameters: {'num_leaves': 254, 'feature_fraction': 0.9684137854091637, 'bagging_fraction': 0.4500172957802525, 'learning_rate': 0.2259038496916121}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:58:46,005]\u001b[0m Trial 58 finished with value: 0.9173055555555556 and parameters: {'num_leaves': 222, 'feature_fraction': 0.9653911546252322, 'bagging_fraction': 0.4488493629032429, 'learning_rate': 0.14696601411196125}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:59:08,926]\u001b[0m Trial 59 finished with value: 0.9207555555555555 and parameters: {'num_leaves': 256, 'feature_fraction': 0.9997079245237979, 'bagging_fraction': 0.4779858214743982, 'learning_rate': 0.2319191320106157}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:59:30,955]\u001b[0m Trial 60 finished with value: 0.9196888888888889 and parameters: {'num_leaves': 204, 'feature_fraction': 0.9888544953935774, 'bagging_fraction': 0.39591982627951444, 'learning_rate': 0.24930126496984334}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 07:59:55,284]\u001b[0m Trial 61 finished with value: 0.9209388888888889 and parameters: {'num_leaves': 256, 'feature_fraction': 0.9996253800116115, 'bagging_fraction': 0.4954968794446896, 'learning_rate': 0.2193536730717655}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:00:16,601]\u001b[0m Trial 62 finished with value: 0.9195555555555556 and parameters: {'num_leaves': 248, 'feature_fraction': 0.9930512224604369, 'bagging_fraction': 0.498366190279464, 'learning_rate': 0.34839467711222927}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:00:30,650]\u001b[0m Trial 63 finished with value: 0.8460333333333333 and parameters: {'num_leaves': 11, 'feature_fraction': 0.9672500982820457, 'bagging_fraction': 0.35759278369658953, 'learning_rate': 0.09636844989107834}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:01:04,383]\u001b[0m Trial 64 finished with value: 0.8889833333333333 and parameters: {'num_leaves': 256, 'feature_fraction': 0.9975826318258341, 'bagging_fraction': 0.41903403530154415, 'learning_rate': 0.03348368833838189}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:01:30,084]\u001b[0m Trial 65 finished with value: 0.9163277777777777 and parameters: {'num_leaves': 230, 'feature_fraction': 0.8766965906999558, 'bagging_fraction': 0.47645762229406075, 'learning_rate': 0.15708031924392318}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:01:47,086]\u001b[0m Trial 66 finished with value: 0.9090444444444444 and parameters: {'num_leaves': 77, 'feature_fraction': 0.9555662356523104, 'bagging_fraction': 0.6685907100976162, 'learning_rate': 0.23382152132381837}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:02:10,743]\u001b[0m Trial 67 finished with value: 0.9179888888888889 and parameters: {'num_leaves': 247, 'feature_fraction': 0.8931501452349307, 'bagging_fraction': 0.5196417924848713, 'learning_rate': 0.253205653714787}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:02:29,578]\u001b[0m Trial 68 finished with value: 0.8920444444444444 and parameters: {'num_leaves': 218, 'feature_fraction': 0.37980251278195676, 'bagging_fraction': 0.43571559088629436, 'learning_rate': 0.13209068847124236}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:02:51,719]\u001b[0m Trial 69 finished with value: 0.9156222222222222 and parameters: {'num_leaves': 231, 'feature_fraction': 0.8496528359440879, 'bagging_fraction': 0.6368343135162181, 'learning_rate': 0.3728449574239801}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:03:13,290]\u001b[0m Trial 70 finished with value: 0.9023388888888889 and parameters: {'num_leaves': 249, 'feature_fraction': 0.47538738375003103, 'bagging_fraction': 0.3594969059106346, 'learning_rate': 0.21052145296354263}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:03:36,995]\u001b[0m Trial 71 finished with value: 0.9204111111111111 and parameters: {'num_leaves': 253, 'feature_fraction': 0.9687892441297815, 'bagging_fraction': 0.47534142618202535, 'learning_rate': 0.22106822982798152}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:04:02,206]\u001b[0m Trial 72 finished with value: 0.9190055555555555 and parameters: {'num_leaves': 236, 'feature_fraction': 0.9443750715971009, 'bagging_fraction': 0.5261375912702511, 'learning_rate': 0.1644715686538653}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:04:26,147]\u001b[0m Trial 73 finished with value: 0.9200111111111111 and parameters: {'num_leaves': 246, 'feature_fraction': 0.9732832850806538, 'bagging_fraction': 0.4814298521515196, 'learning_rate': 0.2631578659233661}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:04:57,347]\u001b[0m Trial 74 finished with value: 0.9156055555555556 and parameters: {'num_leaves': 256, 'feature_fraction': 0.9390937959621598, 'bagging_fraction': 0.4179425396620906, 'learning_rate': 0.09418423356948069}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:05:21,716]\u001b[0m Trial 75 finished with value: 0.9203833333333333 and parameters: {'num_leaves': 248, 'feature_fraction': 0.9996333727249187, 'bagging_fraction': 0.561156581001089, 'learning_rate': 0.23019723856044916}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:05:35,817]\u001b[0m Trial 76 finished with value: 0.8993777777777778 and parameters: {'num_leaves': 37, 'feature_fraction': 0.8979865888305141, 'bagging_fraction': 0.31377830738731755, 'learning_rate': 0.2810528535717318}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:06:00,466]\u001b[0m Trial 77 finished with value: 0.9194611111111111 and parameters: {'num_leaves': 236, 'feature_fraction': 0.9690923170545284, 'bagging_fraction': 0.3060730329909011, 'learning_rate': 0.19414112193033595}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:06:28,807]\u001b[0m Trial 78 finished with value: 0.9178333333333333 and parameters: {'num_leaves': 229, 'feature_fraction': 0.9379426770490129, 'bagging_fraction': 0.394799275177966, 'learning_rate': 0.12708295337606862}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:06:44,407]\u001b[0m Trial 79 finished with value: 0.8793111111111112 and parameters: {'num_leaves': 203, 'feature_fraction': 0.2800808525748845, 'bagging_fraction': 0.3573134603084618, 'learning_rate': 0.160657790926579}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:07:07,291]\u001b[0m Trial 80 finished with value: 0.9155055555555556 and parameters: {'num_leaves': 248, 'feature_fraction': 0.7723351133144937, 'bagging_fraction': 0.4661591167564293, 'learning_rate': 0.33292754916654893}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:07:31,611]\u001b[0m Trial 81 finished with value: 0.9206777777777778 and parameters: {'num_leaves': 249, 'feature_fraction': 0.9998680877496154, 'bagging_fraction': 0.5689709218270642, 'learning_rate': 0.22542348233019383}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:07:55,154]\u001b[0m Trial 82 finished with value: 0.9203833333333333 and parameters: {'num_leaves': 241, 'feature_fraction': 0.9781543085070813, 'bagging_fraction': 0.5018186915140667, 'learning_rate': 0.23049870857392893}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:08:17,835]\u001b[0m Trial 83 finished with value: 0.9187277777777778 and parameters: {'num_leaves': 256, 'feature_fraction': 0.9077045325954767, 'bagging_fraction': 0.5508004343632118, 'learning_rate': 0.29547339996124194}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:08:43,081]\u001b[0m Trial 84 finished with value: 0.9202333333333333 and parameters: {'num_leaves': 218, 'feature_fraction': 0.9422719538947963, 'bagging_fraction': 0.5814634527200707, 'learning_rate': 0.17816720435157055}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:09:05,011]\u001b[0m Trial 85 finished with value: 0.9155388888888889 and parameters: {'num_leaves': 236, 'feature_fraction': 0.971446915470048, 'bagging_fraction': 0.5283498394516818, 'learning_rate': 0.464992925776563}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:09:28,843]\u001b[0m Trial 86 finished with value: 0.9178555555555555 and parameters: {'num_leaves': 249, 'feature_fraction': 0.8756026170134562, 'bagging_fraction': 0.7175001464510425, 'learning_rate': 0.1994149765901025}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:09:51,675]\u001b[0m Trial 87 finished with value: 0.9197611111111111 and parameters: {'num_leaves': 225, 'feature_fraction': 0.948120251711817, 'bagging_fraction': 0.44158528142001746, 'learning_rate': 0.2615883420342927}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:10:24,982]\u001b[0m Trial 88 finished with value: 0.9003722222222222 and parameters: {'num_leaves': 242, 'feature_fraction': 0.9130660021598677, 'bagging_fraction': 0.623087732528528, 'learning_rate': 0.051364774355053516}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:10:53,653]\u001b[0m Trial 89 finished with value: 0.9149166666666667 and parameters: {'num_leaves': 251, 'feature_fraction': 0.8281499171431804, 'bagging_fraction': 0.5017379074627829, 'learning_rate': 0.12154830183947513}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:11:16,511]\u001b[0m Trial 90 finished with value: 0.91975 and parameters: {'num_leaves': 236, 'feature_fraction': 0.9765916543648547, 'bagging_fraction': 0.5771131103409276, 'learning_rate': 0.28845952765255195}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:11:39,788]\u001b[0m Trial 91 finished with value: 0.9214722222222222 and parameters: {'num_leaves': 249, 'feature_fraction': 0.9902522785531154, 'bagging_fraction': 0.5514788275228896, 'learning_rate': 0.2298716663013493}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:12:03,414]\u001b[0m Trial 92 finished with value: 0.9207111111111111 and parameters: {'num_leaves': 245, 'feature_fraction': 0.9958232293284964, 'bagging_fraction': 0.37718716372780814, 'learning_rate': 0.24076524148504647}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:12:24,111]\u001b[0m Trial 93 finished with value: 0.9159555555555555 and parameters: {'num_leaves': 125, 'feature_fraction': 0.9985283231909496, 'bagging_fraction': 0.3743042244079979, 'learning_rate': 0.24236820688679972}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:12:48,929]\u001b[0m Trial 94 finished with value: 0.9195388888888889 and parameters: {'num_leaves': 245, 'feature_fraction': 0.9338090185704545, 'bagging_fraction': 0.3370197249176339, 'learning_rate': 0.18185079624156889}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:13:12,636]\u001b[0m Trial 95 finished with value: 0.9185833333333333 and parameters: {'num_leaves': 240, 'feature_fraction': 0.9564968588490114, 'bagging_fraction': 0.29664165881735566, 'learning_rate': 0.2099783965620397}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:13:41,238]\u001b[0m Trial 96 finished with value: 0.9028111111111111 and parameters: {'num_leaves': 227, 'feature_fraction': 0.6070466974071274, 'bagging_fraction': 0.4066315674407453, 'learning_rate': 0.09156497808717715}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:14:03,600]\u001b[0m Trial 97 finished with value: 0.9072111111111111 and parameters: {'num_leaves': 251, 'feature_fraction': 0.9795159079141189, 'bagging_fraction': 0.25545572009164447, 'learning_rate': 0.665459393765498}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:14:26,610]\u001b[0m Trial 98 finished with value: 0.91605 and parameters: {'num_leaves': 170, 'feature_fraction': 0.9998071091068964, 'bagging_fraction': 0.4310056826984038, 'learning_rate': 0.14773242627418123}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 350272, number of negative: 369728\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4364\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486489 -> initscore=-0.054058\n",
      "[LightGBM] [Info] Start training from score -0.054058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-28 08:14:47,345]\u001b[0m Trial 99 finished with value: 0.8993 and parameters: {'num_leaves': 232, 'feature_fraction': 0.9225277082443837, 'bagging_fraction': 0.598286205857153, 'learning_rate': 0.8263159370802345}. Best is trial 38 with value: 0.9217666666666666.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optimisation_func, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8540677e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T08:14:47.411472Z",
     "iopub.status.busy": "2022-05-28T08:14:47.411006Z",
     "iopub.status.idle": "2022-05-28T08:14:47.416469Z",
     "shell.execute_reply": "2022-05-28T08:14:47.415548Z"
    },
    "papermill": {
     "duration": 0.037856,
     "end_time": "2022-05-28T08:14:47.418390",
     "exception": false,
     "start_time": "2022-05-28T08:14:47.380534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e62cdd0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T08:14:47.476765Z",
     "iopub.status.busy": "2022-05-28T08:14:47.476149Z",
     "iopub.status.idle": "2022-05-28T08:14:47.481459Z",
     "shell.execute_reply": "2022-05-28T08:14:47.480839Z"
    },
    "papermill": {
     "duration": 0.036536,
     "end_time": "2022-05-28T08:14:47.483161",
     "exception": false,
     "start_time": "2022-05-28T08:14:47.446625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 255,\n",
       " 'feature_fraction': 0.9646367375935679,\n",
       " 'bagging_fraction': 0.19053681610791118,\n",
       " 'learning_rate': 0.16513221476967985}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "869a44f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T08:14:47.541943Z",
     "iopub.status.busy": "2022-05-28T08:14:47.541345Z",
     "iopub.status.idle": "2022-05-28T08:15:02.944326Z",
     "shell.execute_reply": "2022-05-28T08:15:02.943273Z"
    },
    "papermill": {
     "duration": 15.43524,
     "end_time": "2022-05-28T08:15:02.946727",
     "exception": false,
     "start_time": "2022-05-28T08:14:47.511487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_features = f27_features(test)\n",
    "test = pd.concat([test, new_features], axis=1)\n",
    "test.drop(columns=[\"f_27\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfe08b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T08:15:03.006660Z",
     "iopub.status.busy": "2022-05-28T08:15:03.006234Z",
     "iopub.status.idle": "2022-05-28T08:15:03.012753Z",
     "shell.execute_reply": "2022-05-28T08:15:03.011906Z"
    },
    "papermill": {
     "duration": 0.038825,
     "end_time": "2022-05-28T08:15:03.014686",
     "exception": false,
     "start_time": "2022-05-28T08:15:02.975861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['kfold', 'id', 'f_07', 'f_08', 'f_09', 'f_10', 'f_11', 'f_12', 'f_13',\n",
       "       'f_14', 'f_15', 'f_16', 'f_17', 'f_18', 'f_29', 'f_30', 'f_00', 'f_01',\n",
       "       'f_02', 'f_03', 'f_04', 'f_05', 'f_06', 'f_19', 'f_20', 'f_21', 'f_22',\n",
       "       'f_23', 'f_24', 'f_25', 'f_26', 'f_28', 'f27_0', 'f27_1', 'f27_2',\n",
       "       'f27_3', 'f27_4', 'f27_5', 'f27_6', 'f27_7', 'f27_8', 'f27_9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "447f7b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T08:15:03.075496Z",
     "iopub.status.busy": "2022-05-28T08:15:03.074871Z",
     "iopub.status.idle": "2022-05-28T08:16:46.841049Z",
     "shell.execute_reply": "2022-05-28T08:16:46.840042Z"
    },
    "papermill": {
     "duration": 103.799378,
     "end_time": "2022-05-28T08:16:46.843599",
     "exception": false,
     "start_time": "2022-05-28T08:15:03.044221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 437839, number of negative: 462161\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4366\n",
      "[LightGBM] [Info] Number of data points in the train set: 900000, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486488 -> initscore=-0.054062\n",
      "[LightGBM] [Info] Start training from score -0.054062\n"
     ]
    }
   ],
   "source": [
    "x = train_folds.iloc[:, 3:].to_numpy()\n",
    "y = train_folds[\"target\"].to_numpy()\n",
    "final_train_set = lightgbm.Dataset(x,y)\n",
    "params = {\n",
    "    'objective':'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'boosting':'gbdt',\n",
    "    'n_estimators':1000,\n",
    "    'num_leaves':best_params[\"num_leaves\"],\n",
    "    'feature_fraction':best_params[\"feature_fraction\"],\n",
    "    'bagging_fraction':best_params[\"bagging_fraction\"],\n",
    "    'learning_rate':best_params[\"learning_rate\"],\n",
    "    'verbose':1\n",
    "}\n",
    "fitted_final_model = lightgbm.train(params, final_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a7f4fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T08:16:46.905156Z",
     "iopub.status.busy": "2022-05-28T08:16:46.904744Z",
     "iopub.status.idle": "2022-05-28T08:17:21.955768Z",
     "shell.execute_reply": "2022-05-28T08:17:21.955031Z"
    },
    "papermill": {
     "duration": 35.105454,
     "end_time": "2022-05-28T08:17:21.979485",
     "exception": false,
     "start_time": "2022-05-28T08:16:46.874031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_pred = []\\nfor pred in ypred:\\n    if pred >= 0.5:\\n        y_pred.append(1)\\n    else:\\n        y_pred.append(0)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ = test.iloc[:, 2:].to_numpy()\n",
    "ypred = fitted_final_model.predict(test_)\n",
    "'''y_pred = []\n",
    "for pred in ypred:\n",
    "    if pred >= 0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41fd03bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T08:17:22.038335Z",
     "iopub.status.busy": "2022-05-28T08:17:22.037906Z",
     "iopub.status.idle": "2022-05-28T08:17:22.045645Z",
     "shell.execute_reply": "2022-05-28T08:17:22.044739Z"
    },
    "papermill": {
     "duration": 0.039929,
     "end_time": "2022-05-28T08:17:22.048002",
     "exception": false,
     "start_time": "2022-05-28T08:17:22.008073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({ 'id': test[\"id\"],\n",
    "                            'target': ypred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdab2735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T08:17:22.108781Z",
     "iopub.status.busy": "2022-05-28T08:17:22.107758Z",
     "iopub.status.idle": "2022-05-28T08:17:24.548509Z",
     "shell.execute_reply": "2022-05-28T08:17:24.547715Z"
    },
    "papermill": {
     "duration": 2.473404,
     "end_time": "2022-05-28T08:17:24.550743",
     "exception": false,
     "start_time": "2022-05-28T08:17:22.077339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c628136",
   "metadata": {
    "papermill": {
     "duration": 0.028523,
     "end_time": "2022-05-28T08:17:24.608123",
     "exception": false,
     "start_time": "2022-05-28T08:17:24.579600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2463.025544,
   "end_time": "2022-05-28T08:17:25.765038",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-28T07:36:22.739494",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
